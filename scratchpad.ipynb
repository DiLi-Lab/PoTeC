{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "to be deleted when everything is done, I'm just using it to the check the data types and the nan values in all csv files\n",
    "+ print the markdown table + some other checks\n",
    "\"\"\"\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PATHS = [\n",
    "    # 'stimuli/aoi_texts/aoi/b0.ias',\n",
    "    'stimuli/word_features/word_features_b0.csv',\n",
    "    'participants/participant_data.csv',\n",
    "    'eyetracking_data/fixations/reader0_b0_fixations.csv',\n",
    "    'eyetracking_data/reader_rm_wf/reader0_b0_merged.csv',\n",
    "    'eyetracking_data/reading_measures/reader0_b0_rm.csv',\n",
    "    'eyetracking_data/scanpaths/reader0_b0_scanpath.csv',\n",
    "    'eyetracking_data/scanpaths_reader_rm_wf/reader0_b0_merged_sp_rm.csv',\n",
    "]\n",
    "\n",
    "PATHS_FOLDERS = [\n",
    "    'stimuli/aoi_texts/aoi/',\n",
    "    'stimuli/word_features/',\n",
    "    'eyetracking_data/fixations/',\n",
    "    'eyetracking_data/reader_rm_wf/',\n",
    "    'eyetracking_data/reading_measures/',\n",
    "    'eyetracking_data/scanpaths/',\n",
    "    'eyetracking_data/scanpaths_reader_rm_wf/',\n",
    "    'stimuli/stimuli/items.tsv',\n",
    "    'stimuli/stimuli/stimuli.tsv',\n",
    "    'participants/',      \n",
    "]\n",
    "\n",
    "floats = ['type_length_chars', \n",
    "          'lemma_length_chars', \n",
    "                     'type_length_syllables', \n",
    "    'annotated_type_frequency_normalized', 'type_frequency_normalized',\n",
    "                     'lemma_frequency_normalized', 'familiarity_normalized', 'regularity_normalized',\n",
    "                     'document_frequency_normalized', 'sentence_frequency_normalized',\n",
    "                     'cumulative_syllable_corpus_frequency_normalized',\n",
    "                     'cumulative_syllable_lexicon_frequency_normalized',\n",
    "                     'cumulative_character_corpus_frequency_normalized',\n",
    "                     'cumulative_character_lexicon_frequency_normalized',\n",
    "                     'cumulative_character_bigram_corpus_frequency_normalized',\n",
    "                     'cumulative_character_bigram_lexicon_frequency_normalized',\n",
    "                     'cumulative_character_trigram_corpus_frequency_normalized',\n",
    "                     'cumulative_character_trigram_lexicon_frequency_normalized',\n",
    "                     'initial_letter_frequency_normalized',\n",
    "                     'initial_bigram_frequency_normalized', 'initial_trigram_frequency_normalized',\n",
    "                     'avg_cond_prob_in_bigrams',\n",
    "                     'avg_cond_prob_in_trigrams', 'neighbors_coltheart_higher_freq_cum_freq_normalized',\n",
    "                     'neighbors_coltheart_higher_freq_count_normalized',\n",
    "                     'neighbors_coltheart_all_cum_freq_normalized',\n",
    "                     'neighbors_coltheart_all_count_normalized',\n",
    "                     'neighbors_levenshtein_higher_freq_cum_freq_normalized',\n",
    "                     'neighbors_levenshtein_higher_freq_count_normalized',\n",
    "                     'neighbors_levenshtein_all_cum_freq_normalized',\n",
    "                     'neighbors_levenshtein_all_count_normalized']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-13T10:18:45.866603Z",
     "start_time": "2023-09-13T10:18:45.859580Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## investigate col values and distribution\n",
    "\n",
    "values = {}\n",
    "for folder in PATHS_FOLDERS:\n",
    "    values[folder] = {}\n",
    "\n",
    "    first = True\n",
    "    suffix = '.ias' if folder == 'stimuli/aoi_texts/aoi/' else '.tsv'\n",
    "    for path in tqdm(Path(folder).glob(f'*{suffix}'), desc=f'Checking files in {folder}'):\n",
    "\n",
    "        csv = pd.read_csv(path, sep='\\t', keep_default_na=False,\n",
    "                          na_values=['#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND',\n",
    "                                     '1.#QNAN', '<NA>', 'N/A', 'NA', 'NaN', 'None', 'n/a', 'nan', ''])\n",
    "        if first:\n",
    "            all_files = csv\n",
    "            first = False\n",
    "        else:\n",
    "            all_files = pd.concat([all_files, csv], ignore_index=True)\n",
    "\n",
    "    print(len(all_files))\n",
    "    cols = all_files.columns.tolist()\n",
    "    for col in all_files.columns:\n",
    "        try:\n",
    "            values[folder][col] += all_files[col].tolist()\n",
    "        except KeyError:\n",
    "            values[folder][col] = all_files[col].tolist()\n",
    "\n",
    "        if all_files[col].dtype in ['float64'] or col in floats:\n",
    "            print(col)\n",
    "            cols.remove(col)\n",
    "            try:\n",
    "                print(f'min: {all_files[col].min()} max: {all_files[col].max()} mean: {all_files[col].mean()} std: {all_files[col].std()}')\n",
    "                all_files[col].astype('float64').plot.kde()\n",
    "                plt.show()\n",
    "\n",
    "            except ValueError:\n",
    "                # print value if value if not numeric\n",
    "                print([v for v in all_files[col].values if not isinstance(v, float)])\n",
    "        \n",
    "        elif all_files[col].dtype in ['int64']:\n",
    "            print(col)\n",
    "            cols.remove(col)            \n",
    "            counts = all_files[col].value_counts()\n",
    "            print(counts.to_dict())\n",
    "            all_files[col].plot.hist()\n",
    "            plt.show()\n",
    "\n",
    "    print(cols)\n",
    "    print(all_files.dtypes)\n",
    "    print(all_files['type_length_chars'].value_counts())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fdb9e4ae8e0f6825"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## check duplicate columns\n",
    "    \n",
    "all_paths = [\n",
    "    'stimuli/word_features',\n",
    "]\n",
    "cols1 = [\n",
    "    'PoS_tag'\n",
    "\n",
    "]\n",
    "cols2 = [\n",
    "    'STTS_PoS_tag'\n",
    "]\n",
    "    \n",
    "for path, col1, col2 in zip(all_paths, cols1, cols2):\n",
    "\n",
    "    files = Path(path).glob('*.csv')\n",
    "\n",
    "    with open(f'stuff_to_check/mismatch_{Path(path).stem}_{col1}_{col2}.txt', 'w', encoding='utf8') as f:\n",
    "\n",
    "        for path in files:\n",
    "\n",
    "            csv = pd.read_csv(path, sep='\\t', na_filter=False)\n",
    "\n",
    "            c1, c2, word = csv[col1], csv[col2], csv['word']\n",
    "\n",
    "            for idx, (v1, v2, w) in enumerate(zip(c1, c2, word)):\n",
    "                if v1.lower() != v2.lower():\n",
    "                    f.write(f'{path}\\n')\n",
    "                    f.write(f'Line: {idx + 1}\\n')\n",
    "                    f.write(f'{col1}: {v1}\\t{col2}: {v2}\\tword:{w}\\n\\n')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b3670b7cfc93652"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
