{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "to be deleted when everything is done, I'm just using it to the check the data types and the nan values in all csv files\n",
    "+ print the markdown table + some other checks\n",
    "\"\"\"\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PATHS = [\n",
    "    # 'stimuli/aoi_texts/aoi/b0.ias',\n",
    "    'stimuli/word_features/word_features_b0.csv',\n",
    "    'participants/participant_data.csv',\n",
    "    'eyetracking_data/fixations/reader0_b0_fixations.csv',\n",
    "    'eyetracking_data/reader_rm_wf/reader0_b0_merged.csv',\n",
    "    'eyetracking_data/reading_measures/reader0_b0_rm.csv',\n",
    "    'eyetracking_data/scanpaths/reader0_b0_scanpath.csv',\n",
    "    'eyetracking_data/scanpaths_reader_rm_wf/reader0_b0_merged_sp_rm.csv',\n",
    "]\n",
    "\n",
    "PATHS_FOLDERS = [\n",
    "    'stimuli/aoi_texts/aoi/',\n",
    "    'stimuli/word_features/',\n",
    "    'eyetracking_data/fixations/',\n",
    "    'eyetracking_data/reader_rm_wf/',\n",
    "    'eyetracking_data/reading_measures/',\n",
    "    'eyetracking_data/scanpaths/',\n",
    "    'eyetracking_data/scanpaths_reader_rm_wf/',\n",
    "    'stimuli/texts_and_questions/',\n",
    "    'participants/',\n",
    "    \n",
    "]\n",
    "\n",
    "floats = ['type_length_chars', \n",
    "          'lemma_length_chars', \n",
    "                     'type_length_syllables', \n",
    "    'annotated_type_frequency_normalized', 'type_frequency_normalized',\n",
    "                     'lemma_frequency_normalized', 'familiarity_normalized', 'regularity_normalized',\n",
    "                     'document_frequency_normalized', 'sentence_frequency_normalized',\n",
    "                     'cumulative_syllable_corpus_frequency_normalized',\n",
    "                     'cumulative_syllable_lexicon_frequency_normalized',\n",
    "                     'cumulative_character_corpus_frequency_normalized',\n",
    "                     'cumulative_character_lexicon_frequency_normalized',\n",
    "                     'cumulative_character_bigram_corpus_frequency_normalized',\n",
    "                     'cumulative_character_bigram_lexicon_frequency_normalized',\n",
    "                     'cumulative_character_trigram_corpus_frequency_normalized',\n",
    "                     'cumulative_character_trigram_lexicon_frequency_normalized',\n",
    "                     'initial_letter_frequency_normalized',\n",
    "                     'initial_bigram_frequency_normalized', 'initial_trigram_frequency_normalized',\n",
    "                     'avg_cond_prob_in_bigrams',\n",
    "                     'avg_cond_prob_in_trigrams', 'neighbors_coltheart_higher_freq_cum_freq_normalized',\n",
    "                     'neighbors_coltheart_higher_freq_count_normalized',\n",
    "                     'neighbors_coltheart_all_cum_freq_normalized',\n",
    "                     'neighbors_coltheart_all_count_normalized',\n",
    "                     'neighbors_levenshtein_higher_freq_cum_freq_normalized',\n",
    "                     'neighbors_levenshtein_higher_freq_count_normalized',\n",
    "                     'neighbors_levenshtein_all_cum_freq_normalized',\n",
    "                     'neighbors_levenshtein_all_count_normalized']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-09T11:03:57.880832Z",
     "start_time": "2023-09-09T11:03:57.875091Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 stimuli/word_features/word_features_p3.tsv\n"
     ]
    }
   ],
   "source": [
    "for path in Path('stimuli/word_features/').glob('*.tsv'):\n",
    "    wf = pd.read_csv(path, sep='\\t', keep_default_na=False,\n",
    "                      na_values=['#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND',\n",
    "                                 '1.#QNAN', '<NA>', 'N/A', 'NA', 'NaN', 'None', 'n/a', 'nan', ''])\n",
    "    \n",
    "    # reaplce values 2 in column is_abbreviation with 0\n",
    "    count = wf['word'].isnull().sum()\n",
    "    if count > 0:\n",
    "        print(count, path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-09T14:26:01.912375Z",
     "start_time": "2023-09-09T14:26:01.864269Z"
    }
   },
   "id": "d4e1ab94edbefca9"
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "# iterate over all folders\n",
    "all_cols = set()\n",
    "    \n",
    "for folder in PATHS_FOLDERS:\n",
    "    # iterate over all tsv files in all folders\n",
    "    \n",
    "    cols = {}\n",
    "    nans = 0\n",
    "    \n",
    "    suffix = '.ias' if folder == 'stimuli/aoi_texts/aoi/' else '.tsv'\n",
    "    for file in Path(folder).glob(f'*{suffix}'):\n",
    "\n",
    "        tsv = pd.read_csv(file, sep='\\t', keep_default_na=False,\n",
    "                          na_values=['#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND',\n",
    "                                     '1.#QNAN', '<NA>', 'N/A', 'NA', 'NaN', 'None', 'n/a', 'nan', ''])\n",
    "        \n",
    "        if type(nans) == int:\n",
    "            nans = tsv.isnull().sum()\n",
    "        else:\n",
    "            nans = nans.add(tsv.isnull().sum(), fill_value=0)\n",
    "        \n",
    "        all_cols.update(tsv.columns)\n",
    "        \n",
    "        for c in tsv.columns:\n",
    "            try:\n",
    "                cols[c]['values'] += tsv[c].tolist()\n",
    "                cols[c]['dtypes'] += [tsv[c].dtype]\n",
    "            except KeyError:\n",
    "                cols[c] = {'values': tsv[c].tolist()}\n",
    "                cols[c]['dtypes'] = [tsv[c].dtype] \n",
    "    \n",
    "    for k, v in cols.items():\n",
    "        v['poss_values'] = []\n",
    "        v['missing_values'] = nans[k] if k in nans else 0\n",
    "        \n",
    "        v['dtypes'] = list(set(v['dtypes']))\n",
    "        if len(v['dtypes']) > 1:\n",
    "            if 'int64' in v['dtypes'] and 'float64' in v['dtypes']:\n",
    "                try:\n",
    "                    v['poss_values'].append(f\"min: {min(v['values'])}, max: {max(v['values'])}, mean: {pd.Series(v['values']).mean()}, std: {pd.Series(v['values']).std()}\")\n",
    "                except TypeError:\n",
    "                    v['poss_values'].append('more than one dtype: ' + str(v['dtypes']))\n",
    "            else:\n",
    "                v['poss_values'].extend(['more than one dtype: ' + str(v['dtypes']), set(v['values'])])\n",
    "        \n",
    "        elif 'object' in v['dtypes'] or '0' in v['dtypes']:\n",
    "            # if set of values is smaller than 10, print the values, otherwise print object\n",
    "            value_set = set(v['values'])\n",
    "            if len(value_set) <= 12 and k not in ['tq_1', 'tq_2', 'tq_3', 'bq_1', 'bq_2', 'bq_3', 'text', 'headline']:\n",
    "                v['poss_values'].append(value_set)\n",
    "            else:\n",
    "                v['poss_values'].append('object')\n",
    "        \n",
    "        elif 'float64' in v['dtypes']:\n",
    "            v['poss_values'].append(f\"min: {min(v['values'])}, max: {max(v['values'])}, mean: {pd.Series(v['values']).mean()}, std: {pd.Series(v['values']).std()}\")\n",
    "        \n",
    "        elif 'int64' in v['dtypes']:\n",
    "            value_set = set(v['values'])\n",
    "            if len(value_set) < 10:\n",
    "                v['poss_values'].append(value_set)\n",
    "            else:\n",
    "                v['poss_values'].append(f\"Interval: {min(v['values'])}-{max(v['values'])}\")\n",
    "        \n",
    "        elif 'bool' in v['dtypes']:\n",
    "            v['poss_values'].append('True/False')\n",
    "        \n",
    "        else:\n",
    "            v['poss_values'].append(v['dtypes'])\n",
    "\n",
    "\n",
    "    df_lists = {'Column name': [], 'Possible values': [], 'Missing value': [], 'Description': [], 'Source': []}\n",
    "    for k, v in cols.items():\n",
    "        df_lists['Column name'].append(k)\n",
    "        df_lists['Possible values'].append(v['poss_values'] if len(v['poss_values']) > 1 else v['poss_values'][0])\n",
    "        df_lists['Missing value'].append(f\"Number of missing values: {v['missing_values']}\")\n",
    "        df_lists['Description'].append(pd.NA)\n",
    "        df_lists['Source'].append(pd.NA)\n",
    "    \n",
    "    df = pd.DataFrame(df_lists)\n",
    "    df.to_csv(f'codebook_tables/{Path(folder).stem}.tsv', sep='\\t', index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-09T20:54:42.959411Z",
     "start_time": "2023-09-09T20:54:23.412759Z"
    }
   },
   "id": "656641b2fd8a2d0e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## investigate col values and distribution\n",
    "\n",
    "values = {}\n",
    "for folder in PATHS_FOLDERS:\n",
    "    values[folder] = {}\n",
    "\n",
    "    first = True\n",
    "    suffix = '.ias' if folder == 'stimuli/aoi_texts/aoi/' else '.tsv'\n",
    "    for path in tqdm(Path(folder).glob(f'*{suffix}'), desc=f'Checking files in {folder}'):\n",
    "\n",
    "        csv = pd.read_csv(path, sep='\\t', keep_default_na=False,\n",
    "                          na_values=['#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND',\n",
    "                                     '1.#QNAN', '<NA>', 'N/A', 'NA', 'NaN', 'None', 'n/a', 'nan', ''])\n",
    "        if first:\n",
    "            all_files = csv\n",
    "            first = False\n",
    "        else:\n",
    "            all_files = pd.concat([all_files, csv], ignore_index=True)\n",
    "\n",
    "    print(len(all_files))\n",
    "    cols = all_files.columns.tolist()\n",
    "    for col in all_files.columns:\n",
    "        try:\n",
    "            values[folder][col] += all_files[col].tolist()\n",
    "        except KeyError:\n",
    "            values[folder][col] = all_files[col].tolist()\n",
    "\n",
    "        if all_files[col].dtype in ['float64'] or col in floats:\n",
    "            print(col)\n",
    "            cols.remove(col)\n",
    "            try:\n",
    "                print(f'min: {all_files[col].min()} max: {all_files[col].max()} mean: {all_files[col].mean()} std: {all_files[col].std()}')\n",
    "                all_files[col].astype('float64').plot.kde()\n",
    "                plt.show()\n",
    "\n",
    "            except ValueError:\n",
    "                # print value if value if not numeric\n",
    "                print([v for v in all_files[col].values if not isinstance(v, float)])\n",
    "        \n",
    "        elif all_files[col].dtype in ['int64']:\n",
    "            print(col)\n",
    "            cols.remove(col)            \n",
    "            counts = all_files[col].value_counts()\n",
    "            print(counts.to_dict())\n",
    "            all_files[col].plot.hist()\n",
    "            plt.show()\n",
    "\n",
    "    print(cols)\n",
    "    print(all_files.dtypes)\n",
    "    print(all_files['type_length_chars'].value_counts())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fdb9e4ae8e0f6825"
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-09T13:20:11.934229Z",
     "start_time": "2023-09-09T13:20:09.213066Z"
    }
   },
   "id": "923dfad6331649a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f8d6c74f632ad68a"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Renaming files in stimuli/word_features/: 12it [00:00, 159.50it/s]\n",
      "Renaming files in eyetracking_data/fixations/: 900it [00:01, 460.42it/s]\n",
      "Renaming files in eyetracking_data/reader_rm_wf/: 900it [00:04, 182.18it/s]\n",
      "Renaming files in eyetracking_data/reading_measures/: 900it [00:01, 473.14it/s]\n",
      "Renaming files in eyetracking_data/scanpaths/: 900it [00:02, 372.90it/s]\n",
      "Renaming files in eyetracking_data/scanpaths_reader_rm_wf/: 900it [00:11, 79.41it/s]\n"
     ]
    }
   ],
   "source": [
    "## rename col names for all files\n",
    "\n",
    "# load the mapping as csv and convert to dict with old value as key and new value as value\n",
    "mapping = pd.read_csv('new_col_mapping.csv')\n",
    "mapping = mapping.fillna('')\n",
    "mapping = dict(zip(mapping['Actual name'], mapping['New name']))\n",
    "\n",
    "for folder in PATHS_FOLDERS:\n",
    "    for path in tqdm(Path(folder).glob('*.csv'), desc=f'Renaming files in {folder}'):\n",
    "\n",
    "        csv = pd.read_csv(path, sep='\\t', keep_default_na=False,\n",
    "                          na_values=['#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND',\n",
    "                                     '1.#QNAN', '<NA>', 'N/A', 'NA', 'NaN', 'None', 'n/a', 'nan', ''])\n",
    "\n",
    "        rename = True\n",
    "        # for col in csv.columns:\n",
    "        #     try:\n",
    "        #         new_name = mapping[col]\n",
    "        #         new_name = col.lower() if not new_name else new_name\n",
    "        # \n",
    "        #         csv.rename(columns={col: new_name}, inplace=True)\n",
    "        #         rename = True\n",
    "        # \n",
    "        #     except KeyError:\n",
    "        #         continue\n",
    "\n",
    "        if rename:\n",
    "            path.rename(path.with_suffix('.tsv'))\n",
    "            csv.to_csv(path, sep='\\t', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-08T09:52:38.464630Z",
     "start_time": "2023-09-08T09:52:15.832330Z"
    }
   },
   "id": "ca6407f534a82625"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## check duplicate columns\n",
    "    \n",
    "all_paths = [\n",
    "    'stimuli/word_features',\n",
    "]\n",
    "cols1 = [\n",
    "    'PoS_tag'\n",
    "\n",
    "]\n",
    "cols2 = [\n",
    "    'STTS_PoS_tag'\n",
    "]\n",
    "    \n",
    "for path, col1, col2 in zip(all_paths, cols1, cols2):\n",
    "\n",
    "    files = Path(path).glob('*.csv')\n",
    "\n",
    "    with open(f'stuff_to_check/mismatch_{Path(path).stem}_{col1}_{col2}.txt', 'w', encoding='utf8') as f:\n",
    "\n",
    "        for path in files:\n",
    "\n",
    "            csv = pd.read_csv(path, sep='\\t', na_filter=False)\n",
    "\n",
    "            c1, c2, word = csv[col1], csv[col2], csv['word']\n",
    "\n",
    "            for idx, (v1, v2, w) in enumerate(zip(c1, c2, word)):\n",
    "                if v1.lower() != v2.lower():\n",
    "                    f.write(f'{path}\\n')\n",
    "                    f.write(f'Line: {idx + 1}\\n')\n",
    "                    f.write(f'{col1}: {v1}\\t{col2}: {v2}\\tword:{w}\\n\\n')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b3670b7cfc93652"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
